# ============================================================================
# Pingora Slice Module - Configuration Example
# ============================================================================
#
# This file demonstrates all available configuration options for the
# Pingora Slice module. The Slice module automatically splits large file
# requests into multiple smaller Range requests to improve caching efficiency
# and reduce origin server load.
#
# For more information, see the README.md file.
# ============================================================================

# ----------------------------------------------------------------------------
# Slice Size Configuration
# ----------------------------------------------------------------------------
# Size of each slice in bytes. This determines how large each Range request
# will be when fetching from the origin server.
#
# Valid range: 64KB (65536) to 10MB (10485760)
# Default: 1MB (1048576)
#
# Tuning guidelines:
# - Smaller slices (256KB-512KB): Better for slow/unreliable networks,
#   more granular caching, but higher overhead
# - Medium slices (1MB-2MB): Good balance for most use cases
# - Larger slices (4MB-10MB): Better for fast networks and large files,
#   but less efficient caching
#
# Example values:
#   slice_size: 262144    # 256KB
#   slice_size: 524288    # 512KB
#   slice_size: 1048576   # 1MB (default)
#   slice_size: 2097152   # 2MB
#   slice_size: 4194304   # 4MB
slice_size: 1048576

# ----------------------------------------------------------------------------
# Concurrency Control
# ----------------------------------------------------------------------------
# Maximum number of concurrent subrequests to the origin server.
# This controls how many slices can be fetched in parallel.
#
# Default: 4
#
# Tuning guidelines:
# - Lower values (2-4): Reduces load on origin server, suitable for
#   bandwidth-limited origins
# - Medium values (4-8): Good balance for most scenarios
# - Higher values (8-16): Maximizes throughput for high-bandwidth origins,
#   but may overwhelm the origin server
#
# Note: Setting this too high may cause the origin server to rate-limit
# or reject requests. Monitor origin server performance when tuning.
max_concurrent_subrequests: 4

# ----------------------------------------------------------------------------
# Retry Configuration
# ----------------------------------------------------------------------------
# Maximum number of retry attempts for failed subrequests.
# When a slice request fails, it will be retried up to this many times
# with exponential backoff before giving up.
#
# Default: 3
#
# Retry backoff schedule: 100ms, 200ms, 400ms, 800ms
#
# Tuning guidelines:
# - 0: No retries, fail immediately (not recommended)
# - 1-2: Minimal retries, faster failure detection
# - 3-5: Recommended for most scenarios
# - 6+: Aggressive retries, may cause delays for clients
max_retries: 3

# ----------------------------------------------------------------------------
# URL Pattern Matching
# ----------------------------------------------------------------------------
# URL patterns that should enable slicing (regex patterns).
# Only requests matching these patterns will use the slice module.
# If the list is empty, all GET requests without Range headers will be
# considered for slicing.
#
# Default: [] (empty list, matches all requests)
#
# Pattern syntax: Standard Rust regex syntax
# - Use ^ to match the beginning of the path
# - Use $ to match the end of the path
# - Use .* to match any characters
# - Use \. to match a literal dot
# - Use (option1|option2) for alternatives
#
# Examples:
slice_patterns:
  # Match all files in /large-files/ directory
  - "^/large-files/.*"
  
  # Match binary files in /downloads/ directory
  - "^/downloads/.*\\.bin$"
  
  # Match video files with common extensions
  - "^/videos/.*\\.(mp4|mkv|avi|mov|wmv)$"
  
  # Match ISO images
  - "^/isos/.*\\.iso$"
  
  # Match compressed archives
  - "^/archives/.*\\.(zip|tar|gz|bz2|7z)$"

# To disable pattern matching and slice all requests, use an empty list:
# slice_patterns: []

# ----------------------------------------------------------------------------
# Cache Configuration
# ----------------------------------------------------------------------------
# Whether to enable caching of slices. When enabled, each slice is cached
# independently, allowing subsequent requests to reuse cached slices.
#
# Default: true
#
# Benefits of caching:
# - Reduces load on origin server
# - Improves response time for repeated requests
# - Enables efficient partial cache hits (some slices cached, others fetched)
#
# Disable caching if:
# - Content changes frequently
# - Storage space is limited
# - Content is already cached upstream
enable_cache: true

# Cache TTL (Time To Live) in seconds.
# How long cached slices should be kept before expiring.
#
# Default: 3600 (1 hour)
#
# Common values:
#   cache_ttl: 300      # 5 minutes (frequently changing content)
#   cache_ttl: 1800     # 30 minutes
#   cache_ttl: 3600     # 1 hour (default)
#   cache_ttl: 7200     # 2 hours
#   cache_ttl: 86400    # 24 hours (static content)
#   cache_ttl: 604800   # 7 days (rarely changing content)
cache_ttl: 3600

# ----------------------------------------------------------------------------
# Two-Tier Cache Configuration (L1 + L2)
# ----------------------------------------------------------------------------
# Pingora Slice supports a two-tier caching system:
# - L1 Cache: In-memory cache for fast access to hot data
# - L2 Cache: Disk-based cache for persistence and cold data
#
# Benefits:
# - L1 provides microsecond-level access for frequently accessed slices
# - L2 provides persistence across restarts and larger storage capacity
# - Automatic promotion: L2 hits are promoted to L1 for faster future access
# - LRU eviction: Least recently used items are evicted from L1 when full

# L1 (Memory) cache size in bytes
# This is the maximum amount of RAM used for caching hot slices.
#
# Default: 104857600 (100MB)
#
# Tuning guidelines:
# - Small deployments: 50-100MB
# - Medium deployments: 100-500MB
# - Large deployments: 500MB-2GB
# - Consider available RAM and concurrent request patterns
#
# Examples:
#   l1_cache_size_bytes: 52428800    # 50MB
#   l1_cache_size_bytes: 104857600   # 100MB (default)
#   l1_cache_size_bytes: 524288000   # 500MB
#   l1_cache_size_bytes: 1073741824  # 1GB
l1_cache_size_bytes: 104857600

# L2 (Disk) cache directory
# Directory where cached slices are stored on disk for persistence.
#
# Default: "/var/cache/pingora-slice"
#
# Requirements:
# - Directory must be writable by the pingora-slice process
# - Sufficient disk space for cache_ttl * request_rate * slice_size
# - Fast storage (SSD/NVMe) recommended for better performance
#
# Examples:
#   l2_cache_dir: "/var/cache/pingora-slice"     # Default
#   l2_cache_dir: "/mnt/ssd/pingora-cache"       # SSD mount
#   l2_cache_dir: "/mnt/nvme/pingora-cache"      # NVMe for best performance
l2_cache_dir: "/var/cache/pingora-slice"

# Whether to enable L2 (disk) cache
# When enabled, slices are persisted to disk and survive restarts.
#
# Default: true
#
# Enable L2 cache when:
# - You want cache persistence across restarts
# - You have sufficient disk space
# - You want to reduce origin server load
#
# Disable L2 cache when:
# - Disk space is very limited
# - Content changes extremely frequently
# - You prefer pure in-memory caching
enable_l2_cache: true

# ----------------------------------------------------------------------------
# Upstream Server Configuration
# ----------------------------------------------------------------------------
# Upstream origin server address where the actual content is hosted.
# The slice module will send Range requests to this server.
#
# Format: "host:port" or "ip:port"
# Default: "127.0.0.1:8080"
#
# Examples:
#   upstream_address: "origin.example.com:80"
#   upstream_address: "192.168.1.100:8080"
#   upstream_address: "backend.internal:3000"
upstream_address: "origin.example.com:80"

# ----------------------------------------------------------------------------
# Metrics Endpoint Configuration (Optional)
# ----------------------------------------------------------------------------
# Configuration for the HTTP metrics endpoint that exposes runtime statistics
# in Prometheus format. This is useful for monitoring and observability.
#
# If this section is omitted or set to null, the metrics endpoint will be
# disabled.
#
# Exposed metrics include:
# - Total requests and sliced requests
# - Cache hit/miss rates
# - Subrequest counts and latencies
# - Error rates
# - Bytes transferred (from origin, from cache, to client)
metrics_endpoint:
  # Whether to enable the metrics endpoint
  # Default: false
  enabled: true
  
  # Address to bind the metrics HTTP server to
  # Format: "host:port"
  # Default: "127.0.0.1:9090"
  #
  # Security considerations:
  # - Binding to 127.0.0.1 (localhost) restricts access to local machine only
  # - Binding to 0.0.0.0 allows access from any network interface
  # - Use firewall rules or reverse proxy for external access
  # - Consider authentication if exposing externally
  #
  # Examples:
  #   address: "127.0.0.1:9090"  # Local access only (recommended)
  #   address: "0.0.0.0:9090"    # All interfaces (use with caution)
  #   address: "10.0.0.5:9090"   # Specific internal IP
  address: "127.0.0.1:9090"

# To disable the metrics endpoint, either omit the section entirely or set:
# metrics_endpoint: null

# ============================================================================
# Configuration Examples for Different Scenarios
# ============================================================================

# Example 1: High-performance setup for fast networks
# ----------------------------------------------------------------------------
# slice_size: 4194304                    # 4MB slices
# max_concurrent_subrequests: 8          # Higher concurrency
# max_retries: 2                         # Fewer retries
# cache_ttl: 86400                       # 24 hour cache

# Example 2: Conservative setup for slow/unreliable networks
# ----------------------------------------------------------------------------
# slice_size: 262144                     # 256KB slices
# max_concurrent_subrequests: 2          # Lower concurrency
# max_retries: 5                         # More retries
# cache_ttl: 3600                        # 1 hour cache

# Example 3: Minimal caching for frequently changing content
# ----------------------------------------------------------------------------
# slice_size: 1048576                    # 1MB slices
# max_concurrent_subrequests: 4          # Standard concurrency
# max_retries: 3                         # Standard retries
# cache_ttl: 300                         # 5 minute cache

# Example 4: Disable slicing for specific patterns
# ----------------------------------------------------------------------------
# slice_patterns:
#   - "^/api/.*"                         # Only slice API endpoints
# Or use an empty list to slice everything:
# slice_patterns: []

# ----------------------------------------------------------------------------
# Cache Purge Configuration (Optional)
# ----------------------------------------------------------------------------
# Configuration for HTTP PURGE method support, allowing cache invalidation
# via standard HTTP PURGE requests.
#
# If this section is omitted, PURGE functionality will be disabled.
#
# PURGE methods supported:
# - PURGE /path/to/file - Purge specific URL
# - PURGE /* with X-Purge-All: true - Purge all cache
# - PURGE /path/* with X-Purge-Pattern: prefix - Purge by prefix
purge:
  # Whether to enable PURGE functionality
  # Default: false
  enabled: true
  
  # Authentication token for PURGE requests (optional)
  # If set, PURGE requests must include this token in the Authorization header:
  #   Authorization: Bearer your-secret-token-here
  # Or in the X-Purge-Token header:
  #   X-Purge-Token: your-secret-token-here
  #
  # If not set or null, PURGE requests will not require authentication.
  # WARNING: In production, always use authentication to prevent unauthorized
  # cache purging!
  #
  # Example:
  #   auth_token: "your-secret-token-here"
  # To disable authentication (NOT recommended for production):
  #   auth_token: null
  auth_token: "your-secret-token-here"
  
  # Whether to enable Prometheus metrics for PURGE operations
  # Default: true
  #
  # Metrics exposed:
  # - pingora_slice_purge_requests_total - Total PURGE requests by method
  # - pingora_slice_purge_requests_by_result - Requests by result (success/failure)
  # - pingora_slice_purge_items_total - Total items purged
  # - pingora_slice_purge_duration_seconds - PURGE operation duration
  # - pingora_slice_purge_auth_failures_total - Authentication failures
  enable_metrics: true

# To disable PURGE functionality, either omit the section entirely or set:
# purge: null

# Example PURGE requests:
# ----------------------------------------------------------------------------
# # Purge specific URL
# curl -X PURGE http://your-server.com/path/to/file.dat \
#   -H "Authorization: Bearer your-secret-token-here"
#
# # Purge all cache
# curl -X PURGE http://your-server.com/* \
#   -H "X-Purge-All: true" \
#   -H "Authorization: Bearer your-secret-token-here"
#
# # Purge by URL prefix
# curl -X PURGE http://your-server.com/videos/movie.mp4 \
#   -H "X-Purge-Pattern: prefix" \
#   -H "Authorization: Bearer your-secret-token-here"

# ============================================================================
# End of Configuration
# ============================================================================
